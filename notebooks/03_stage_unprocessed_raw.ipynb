{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b62b831-c06a-425e-b344-e5deb217a404",
   "metadata": {},
   "source": [
    "# 03_stage_unprocessed_raw.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2dff11-fb2f-4b98-9269-a1f879d250d8",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "\n",
    "This notebook implements the **staging layer** of the Raycon Google Shopping data pipeline.\n",
    "\n",
    "Its purpose is to **incrementally process newly ingested raw API responses** and transform them into structured, analysis-ready staging tables.\n",
    "\n",
    "Specifically, this pipeline:\n",
    "\n",
    "- Identifies raw search records that have **not yet been staged**\n",
    "- Flattens and normalizes semi-structured Google Shopping JSON responses\n",
    "- Handles the presence or absence of different result sections (uncategorized, categorized, and inline results)\n",
    "- Inserts the transformed data into the `raycon.stg_searches` and `raycon.stg_results` tables\n",
    "\n",
    "The notebook is designed to be **repeatable and safe to re-run**, ensuring that previously staged data is not duplicated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ba483-f653-4e14-bd83-a750d00080c7",
   "metadata": {},
   "source": [
    "## 2. Setup and Configuration\n",
    "This section loads required libraries and establishes a connection to the PostgreSQL database using environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec1c01-9ace-402f-ba03-f97dad5ab018",
   "metadata": {},
   "source": [
    "### 2.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f19e6a92-34ca-4ae3-8182-29b192c73447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "import json\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30c506-05f8-4cb9-8836-c55af51226cc",
   "metadata": {},
   "source": [
    "## 2.2 Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eb8c4e1-4346-4743-82ce-b0f7dfbbb74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pulled_at</th>\n",
       "      <th>keyword</th>\n",
       "      <th>page</th>\n",
       "      <th>response_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>2025-12-15 19:04:24.094346+00:00</td>\n",
       "      <td>bluetooth headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>{'filters': [{'type': 'Refine results', 'optio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>2025-12-15 19:04:13.742109+00:00</td>\n",
       "      <td>wireless earbuds</td>\n",
       "      <td>1</td>\n",
       "      <td>{'filters': [{'type': 'Refine results', 'optio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96</td>\n",
       "      <td>2025-12-15 19:04:01.556488+00:00</td>\n",
       "      <td>wireless headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>{'filters': [{'type': 'Refine results', 'optio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                        pulled_at               keyword  page  \\\n",
       "0  98 2025-12-15 19:04:24.094346+00:00  bluetooth headphones     1   \n",
       "1  97 2025-12-15 19:04:13.742109+00:00      wireless earbuds     1   \n",
       "2  96 2025-12-15 19:04:01.556488+00:00   wireless headphones     1   \n",
       "\n",
       "                                       response_json  \n",
       "0  {'filters': [{'type': 'Refine results', 'optio...  \n",
       "1  {'filters': [{'type': 'Refine results', 'optio...  \n",
       "2  {'filters': [{'type': 'Refine results', 'optio...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "user = os.getenv('PGUSER')\n",
    "password = os.getenv('PGPASSWORD')\n",
    "pghost = os.getenv('PGHOST')\n",
    "pgport = os.getenv('PGPORT')\n",
    "pgdatabase = os.getenv('PGDATABASE')\n",
    "\n",
    "engine = create_engine(\n",
    "    f'postgresql+psycopg2://{user}:{password}@{pghost}:{pgport}/{pgdatabase}')\n",
    "\n",
    "# Verify database connection by querying recent raw records\n",
    "pd.read_sql(\"SELECT * FROM raycon.raw_google_shopping ORDER BY id DESC LIMIT 3\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ce64f-151f-4834-8927-02081d0045b0",
   "metadata": {},
   "source": [
    "## 3. Pull Raw JSON Records Not Yet Staged\n",
    "This step identifies raw Google Shopping search records that have not yet been processed into the staging tables.\n",
    "Only raw records whose `id` does not appear in `raycon.stg_searches` are selected, allowing the staging pipeline to run incrementally without duplicating previously staged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d4bf154-6a5c-4065-9564-5f699f09d75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pulled_at</th>\n",
       "      <th>keyword</th>\n",
       "      <th>page</th>\n",
       "      <th>response_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>2025-12-15 19:04:01.556488+00:00</td>\n",
       "      <td>wireless headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>{'filters': [{'type': 'Refine results', 'optio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97</td>\n",
       "      <td>2025-12-15 19:04:13.742109+00:00</td>\n",
       "      <td>wireless earbuds</td>\n",
       "      <td>1</td>\n",
       "      <td>{'filters': [{'type': 'Refine results', 'optio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>98</td>\n",
       "      <td>2025-12-15 19:04:24.094346+00:00</td>\n",
       "      <td>bluetooth headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>{'filters': [{'type': 'Refine results', 'optio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                        pulled_at               keyword  page  \\\n",
       "4  96 2025-12-15 19:04:01.556488+00:00   wireless headphones     1   \n",
       "5  97 2025-12-15 19:04:13.742109+00:00      wireless earbuds     1   \n",
       "6  98 2025-12-15 19:04:24.094346+00:00  bluetooth headphones     1   \n",
       "\n",
       "                                       response_json  \n",
       "4  {'filters': [{'type': 'Refine results', 'optio...  \n",
       "5  {'filters': [{'type': 'Refine results', 'optio...  \n",
       "6  {'filters': [{'type': 'Refine results', 'optio...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query and load all raw rows that have not yet been staged\n",
    "query = '''\n",
    "SELECT id, pulled_at, keyword, page, response_json\n",
    "FROM raycon.raw_google_shopping\n",
    "WHERE id NOT IN (SELECT raw_id FROM raycon.stg_searches)\n",
    "ORDER BY pulled_at;\n",
    "'''\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df_raw = pd.read_sql(query, conn)\n",
    "df_raw.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a8879-485a-438f-8bff-17d9486932cb",
   "metadata": {},
   "source": [
    "## 4. Helper Functions\n",
    "\n",
    "This section contains helper functions used to transform raw SerpAPI JSON\n",
    "responses into staging-ready search- and product-level DataFrames.\n",
    "\n",
    "The logic here is currently copied from `02_parse_raw` and defined inline to\n",
    "support iterative development and debugging. These functions are written as\n",
    "pure transformations (no database writes or side effects) and will be\n",
    "extracted into reusable `.py` modules in a later refactor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec02c9d-6e8e-4777-91fe-f7cebb19d26e",
   "metadata": {},
   "source": [
    "### 4.1 build_searches_for_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ff742a7-87f9-4de8-b36b-1eeab10fa957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_searches_for_keyword(keyword_raw):\n",
    "    \"\"\"\n",
    "    Build the stg_searches record for a single raw search event.\n",
    "\n",
    "    This function:\n",
    "    - Takes the first row of df_raw (prototype mode)\n",
    "    - Extracts identifiers (raw_id, pulled_at, keyword, page)\n",
    "    - Extracts search parameters from response_json[\"search_parameters\"]\n",
    "    - Places everything into a clean, single-row DataFrame\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: one clean stg_search row\n",
    "    \"\"\"\n",
    "    \n",
    "     # Grab the first raw record (example row in this case)\n",
    "    search_row = keyword_raw\n",
    "\n",
    "     # Pull identifiers & search metadata from the raw table\n",
    "    raw_id, pulled_at, keyword, page = search_row[['id', 'pulled_at', 'keyword', 'page']]\n",
    "    \n",
    "    # Extract the \"search_parameters\" object from the JSON\n",
    "    params = search_row[\"response_json\"][\"search_parameters\"]\n",
    "\n",
    "    # Build the clean staging DataFrame\n",
    "    search_df_clean = pd.DataFrame([{\n",
    "    \"raw_id\": raw_id,\n",
    "    \"pulled_at\": pulled_at,\n",
    "    \"keyword\": keyword,\n",
    "    \"page\": page,\n",
    "    \"location_used\": params.get(\"location_used\"),\n",
    "    \"location_requested\": params.get(\"location_requested\"),\n",
    "    \"gl\": params.get(\"gl\"),\n",
    "    \"hl\": params.get(\"hl\"),\n",
    "    \"device\": params.get(\"device\"),\n",
    "    \"num_results_requested\": int(params.get(\"num\")),\n",
    "    \"engine\": params.get(\"engine\"),\n",
    "    \"google_domain\": params.get(\"google_domain\"),\n",
    "    }])\n",
    "    return search_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f03d4-3795-461a-871b-8198fd088343",
   "metadata": {},
   "source": [
    "### 4.2 transform_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ff3ff50-2528-44b8-855d-0af641a12261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_results_df(\n",
    "    df_in,\n",
    "    *,\n",
    "    raw_id,\n",
    "    keyword,\n",
    "    page,\n",
    "    pulled_at,\n",
    "    module_type,\n",
    "    module_label,\n",
    "    module_index):\n",
    "    \"\"\"\n",
    "    Take a raw shopping-results DataFrame for a single module\n",
    "    and return a cleaned, standardized results DataFrame.\n",
    "\n",
    "    - Drops unused SerpAPI fields\n",
    "    - Normalizes `multiple_sources` to boolean\n",
    "    - Renames extracted price fields\n",
    "    - Adds module + search context columns\n",
    "    - Reorders columns to match `stg_results` schema\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the original input\n",
    "    transform_df = df_in.copy()\n",
    "\n",
    "    # 1) Drop columns we decided not to stage\n",
    "    transform_df = transform_df.drop(columns=['snippet', 'price', 'delivery', 'old_price', 'thumbnail',\n",
    "                                             'extensions', 'source_icon', 'product_link', 'serpapi_thumbnail', \n",
    "                                            'immersive_product_page_token', 'serpapi_immersive_product_api'],\n",
    "                                    errors='ignore')\n",
    "\n",
    "    # 2) Convert multiple_sources -> proper bool\n",
    "    if 'multiple_sources' in transform_df:\n",
    "        transform_df['multiple_sources'] = (\n",
    "            transform_df['multiple_sources'].replace('True', True).fillna(False).astype(bool))\n",
    "    else:\n",
    "        transform_df['multiple_sources'] = False\n",
    "\n",
    "    # 3) Create and nullify block_position for results which lack this field\n",
    "    transform_df[\"block_position\"] = transform_df.get(\"block_position\", None)\n",
    "\n",
    "    # 4) Rename fields to final names\n",
    "    transform_df = transform_df.rename(columns={\n",
    "        'position': 'position_in_module',\n",
    "        'extracted_price': 'price',\n",
    "        'extracted_old_price': 'old_price'\n",
    "    })\n",
    "\n",
    "    # 5) Attach module + search context\n",
    "    transform_df = transform_df.assign(\n",
    "        module_type=module_type,\n",
    "        module_label=module_label,\n",
    "        module_index=module_index,\n",
    "        raw_id=raw_id,\n",
    "        keyword=keyword,\n",
    "        page=page,\n",
    "        pulled_at=pulled_at\n",
    "    )\n",
    "\n",
    "    # 6) Reorder columns to match target table\n",
    "    final_cols = [\n",
    "    \"raw_id\",\n",
    "    \"keyword\",\n",
    "    \"page\",\n",
    "    \"pulled_at\",\n",
    "    \"title\",\n",
    "    \"product_id\",\n",
    "    \"price\",\n",
    "    \"old_price\",\n",
    "    \"reviews\",\n",
    "    \"rating\",\n",
    "    \"source\",\n",
    "    \"multiple_sources\",\n",
    "    \"tag\",\n",
    "    \"module_type\",\n",
    "    \"module_label\",\n",
    "    \"module_index\",\n",
    "    \"block_position\",\n",
    "    \"position_in_module\"\n",
    "    ]\n",
    "    transform_df = transform_df.reindex(columns=final_cols)\n",
    "    \n",
    "    return transform_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321fb45c-b96b-4f74-b2f4-25e1fd235d7c",
   "metadata": {},
   "source": [
    "### 4.3 build_results_for_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47b5d56a-6fe6-473b-a23a-f9e0fbb5ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_results_for_keyword(keyword_raw):\n",
    "    \"\"\"\n",
    "    Build a full results DataFrame for ONE raw keyword request row.\n",
    "\n",
    "    Steps:\n",
    "    - Pull raw_id / keyword / page / pulled_at from df_raw\n",
    "    - Flatten `shopping_results` (all products)\n",
    "    - Flatten each entry in `categorized_shopping_results`\n",
    "    - Run everything through `transform_results_df`\n",
    "    - Union uncategorized + categorized into one DataFrame\n",
    "    \"\"\"\n",
    "     # 1) Grab the first raw row (later this will be parameterized / looped)\n",
    "    keyword_search = keyword_raw\n",
    "\n",
    "     # 2) Extract search metadata used for context columns\n",
    "    raw_id, pulled_at, keyword, page = keyword_search.loc[['id', 'pulled_at', 'keyword', 'page']]\n",
    "\n",
    "    # 3) Extract JSON extract from the keyword request\n",
    "    keyword_json = keyword_search.loc['response_json']\n",
    "\n",
    "    # 4) Create a blank dataframe list\n",
    "    df_list = []\n",
    "\n",
    "    # 5) Build uncategorized results\n",
    "    uncategorized_df_raw = pd.DataFrame(keyword_json['shopping_results'])\n",
    "    uncategorized_df_clean = transform_results_df(uncategorized_df_raw, \n",
    "                                                raw_id=raw_id, pulled_at=pulled_at, keyword=keyword, page=page,\n",
    "                                                module_type='all_products', module_label='All products', module_index=99)\n",
    "    \n",
    "    df_list.append(uncategorized_df_clean)\n",
    "\n",
    "    # 6) (If they exist) Loop over categorized modules, flatten + transform each\n",
    "    categorized_df_raw = pd.DataFrame(keyword_json.get('categorized_shopping_results', []))\n",
    "    if categorized_df_raw.empty:\n",
    "        categorized_df_clean = categorized_df_raw\n",
    "    else:\n",
    "        for i in range(len(categorized_df_raw)):\n",
    "            category_title = categorized_df_raw['title'][i]\n",
    "            per_category_results_raw = pd.json_normalize(categorized_df_raw['shopping_results'][i])\n",
    "            per_category_results_clean = transform_results_df(per_category_results_raw,\n",
    "                                                             raw_id=raw_id, pulled_at=pulled_at, keyword=keyword, page=page,\n",
    "                                                             module_type='categorized_products', module_label=categorized_df_raw['title'][i], module_index=i+1)\n",
    "            if i == 0:\n",
    "                categorized_df_clean = per_category_results_clean.copy()\n",
    "            else:\n",
    "                categorized_df_clean = (pd.concat([categorized_df_clean, per_category_results_clean], \n",
    "                                            ignore_index=True))\n",
    "            \n",
    "    df_list.append(categorized_df_clean)\n",
    "\n",
    "    #7) (If they exist) Build inline results\n",
    "    inline_df_raw = pd.DataFrame(keyword_json.get('inline_shopping_results', []))\n",
    "    if inline_df_raw.empty:\n",
    "                inline_df_clean = inline_df_raw\n",
    "    else:\n",
    "        inline_df_clean = transform_results_df(inline_df_raw, \n",
    "                                                raw_id=raw_id, pulled_at=pulled_at, keyword=keyword, page=page,\n",
    "                                                module_type='inline_products', module_label='Inline products', module_index=100)\n",
    "    \n",
    "    df_list.append(inline_df_clean)\n",
    "\n",
    "    # 8) Return union of uncategorized + categorized (if exists) + inline (if exists) into one df\n",
    "    return pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d5302-9e63-47b9-813e-ec4827eeade6",
   "metadata": {},
   "source": [
    "## 5. Build Staging DataFrames from Unprocessed Searches\n",
    "\n",
    "Iterate over all raw search rows that have not yet been staged and transform\n",
    "each into:\n",
    "\n",
    "- One search-level staging record (`stg_searches`)\n",
    "- A set of product-level staging records (`stg_results`)\n",
    "\n",
    "Results are accumulated in memory and concatenated once per table to avoid\n",
    "repeated database writes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a537ab83-4e14-4239-abe4-157bbef75444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store the staged results of each raw record\n",
    "search_rows = []\n",
    "results_dfs = []\n",
    "\n",
    "# Append each raw record to its respective list\n",
    "for _, raw_row in df_raw.iterrows():\n",
    "    search_rows.append(build_searches_for_keyword(raw_row))\n",
    "    results_dfs.append(build_results_for_keyword(raw_row))\n",
    "\n",
    "# Union the lists' dataframes together to create the final staged dataframes\n",
    "stg_searches_df = pd.concat(search_rows, ignore_index=True)\n",
    "stg_results_df = pd.concat(results_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fdc97d-0688-48d5-a926-c4e5bbe50742",
   "metadata": {},
   "source": [
    "## 6. Persist Staged Data\n",
    "\n",
    "Append the transformed search-level and product-level DataFrames to their\n",
    "corresponding staging tables within a single database transaction to ensure\n",
    "atomicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e7f9f96-f87b-48ce-9e0e-f85b31aacefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist staged data atomically (either both tables succeed or both fail)\n",
    "with engine.begin() as conn:\n",
    "    stg_searches_df.to_sql(\n",
    "        \"stg_searches\", conn, schema=\"raycon\",\n",
    "        if_exists=\"append\", index=False)\n",
    "\n",
    "    stg_results_df.to_sql(\n",
    "        \"stg_results\", conn, schema=\"raycon\",\n",
    "        if_exists=\"append\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
